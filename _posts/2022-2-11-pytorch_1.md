---
layout: post
title: 'Pytorchä¾¿ç­¾'
date: 2022-2-11
author: ä¸æ˜¾ç”µæ€§
cover: 'http://commcheck396.github.io/assets/img/2022_1_26/pytorch.jpg'
tags: ML Python Pytorch
---

# ~~ä¸èƒ½å†æ‹–äº†~~

> Pythorchï¼Œæ±Ÿæ¹–äººç§°å°Numpy

[Pytorchå¸¸è§è¿ç®—](https://pytorch.org/docs/stable/torch.html)

<br/>

## Mostly Used

Numpyå‘Torchçš„è½¬åŒ–
`torch_data = torch.from_numpy(np_data)`

<br/>

Torchçš„çŸ©é˜µä¹˜æ³•
```python

torch.mm(tensor, tensor)
# 'mm' stands for 'matrix multiply'
# æ³¨æ„æ­¤å¤„ä¸Numpyçš„åŒºåˆ«ï¼Œä¸èƒ½ç›´æ¥åˆ©ç”¨.dot()è¿›è¡Œè¿ç®— 

```

<br/>  

Trochçš„æ•°å­¦è¿ç®—è§„åˆ™å‡ ä¹ä¸Numpyä¸€è‡´ï¼Œ[è¯·å‚è€ƒ](https://pytorch.org/docs/stable/torch.html)

<br/>

Torchä¸­ï¼Œè‹¥è¦è¿›è¡Œåå‘ä¼ æ’­ï¼Œéœ€è¦åˆ©ç”¨variableè¿›è¡Œè¿ç®—ï¼Œvariableå¯ä»¥ä¸€æ¬¡æ€§å°†æ‰€æœ‰ä¿®æ”¹å¹…åº¦ (æ¢¯åº¦) éƒ½è®¡ç®—å‡ºæ¥, è€Œtensorå°±æ²¡æœ‰è¿™ä¸ªèƒ½åŠ›ã€‚  
ä½†variableä¸­çš„æ•°æ®éœ€è¦tensorç±»å‹å¯¼å…¥ï¼š
```python 
tensor = torch.FloatTensor([[1,2],[3,4]])
# requires_gradæ˜¯å‚ä¸å‚ä¸è¯¯å·®åå‘ä¼ æ’­, è¦ä¸è¦è®¡ç®—æ¢¯åº¦
variable = Variable(tensor, requires_grad=True)
```

å‡ç»´
```python
x=torch.unsqueeze(x,dim=1)
```

## æ¿€åŠ±å‡½æ•°

Torch ä¸­çš„æ¿€åŠ±å‡½æ•°æœ‰å¾ˆå¤š, ä¸è¿‡æˆ‘ä»¬å¹³æ—¶è¦ç”¨åˆ°çš„å°±è¿™å‡ ä¸ªï¼š **`relu`**, `sigmoid`, `tanh`, `softplus`ã€‚ 
å¦‚ä½•åˆ©ç”¨æ¿€åŠ±å‡½æ•°:
```python
# following are popular activation functions
y_relu = torch.relu(x).numpy()
y_sigmoid = torch.sigmoid(x).numpy()
y_tanh = torch.tanh(x).numpy()
y_softplus = F.softplus(x).numpy() # there's no softplus in torch
```

## ç¥ç»ç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ

### æ™®é€šæ­å»ºæ–¹æ³•ï¼š  

```python
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output): # åˆ†åˆ«ä»£è¡¨è¾“å…¥ï¼Œè¯¥å±‚ç¥ç»å…ƒä¸ªæ•°ï¼Œè¾“å‡º
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.predict = torch.nn.Linear(n_hidden, n_output)

    def forward(self, x):
        x = F.relu(self.hidden(x)) # åˆ©ç”¨reluæ¿€åŠ±å‡½æ•°
        x = self.predict(x)
        return x

net = Net(1, 10, 1) # æ­å»ºå«æœ‰ä¸€å±‚åä¸ªhiddenç¥ç»å…ƒçš„ç¥ç»ç½‘ç»œ

# ç»“æ„ï¼š
# Net (
#   (hidden): Linear (1 -> 10)
#   (predict): Linear (10 -> 1)
# )

```

~~ğŸ¶éƒ½ä¸ç”¨ï¼Œæœ‰è½®å­è¿˜ä¸ç”¨ï¼Ÿ~~  
 <br/>

### ~~æˆ‘tmç›´æ¥~~å¿«é€Ÿæ­å»º
```python
net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(), # æ³¨æ„è¦å¤§å†™ï¼Œè¿™é‡Œçš„ReLUæ˜¯ä¸€ä¸ªclass
    torch.nn.Linear(10, 1)
)

# ç»“æ„ï¼š
# Sequential (
#   (0): Linear (1 -> 10)
#   (1): ReLU ()
#   (2): Linear (10 -> 1)
# )
```

### è®­ç»ƒæ–¹æ³•  

#### å›å½’æ‹Ÿåˆ
```python
# optimizer æ˜¯è®­ç»ƒçš„å·¥å…·
optimizer = torch.optim.SGD(net.parameters(), lr=0.2)  # ä¼ å…¥ net çš„æ‰€æœ‰å‚æ•°, learning rate
loss_func = torch.nn.MSELoss()      # é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„è¯¯å·®è®¡ç®—å…¬å¼ (å‡æ–¹å·®)

for t in range(100):
    prediction = net(x)     # å–‚ç»™ net è®­ç»ƒæ•°æ® x, è¾“å‡ºé¢„æµ‹å€¼
    loss = loss_func(prediction, y)     # è®¡ç®—ä¸¤è€…çš„è¯¯å·®,predictionè¦åœ¨å‰
    optimizer.zero_grad()   # æ¸…ç©ºä¸Šä¸€æ­¥çš„æ®‹ä½™æ›´æ–°å‚æ•°å€¼
    loss.backward()         # è¯¯å·®åå‘ä¼ æ’­, è®¡ç®—å‚æ•°æ›´æ–°å€¼
    optimizer.step()        # å°†å‚æ•°æ›´æ–°å€¼æ–½åŠ åˆ° net çš„ parameters ä¸Š
```

#### åŒºåˆ†ç±»å‹
```python
net = Net(n_feature=2, n_hidden=10, n_output=2) # å‡ ä¸ªç±»åˆ«å°±å‡ ä¸ª output

# optimizer æ˜¯è®­ç»ƒçš„å·¥å…·
optimizer = torch.optim.SGD(net.parameters(), lr=0.02)  # ä¼ å…¥ net çš„æ‰€æœ‰å‚æ•°,learning rate
loss_func = torch.nn.CrossEntropyLoss() 
# æ³¨æ„loss functionçš„æ”¹å˜ï¼Œæ­¤æ—¶çš„è¾“å‡ºæ˜¯äºŒç»´æ•°æ®ï¼Œæ¯”å¦‚[0.2,0.8]ï¼Œä»£è¡¨è¯¥ç‚¹å±äºç¬¬ä¸€ç±»åˆ«çš„æ¦‚ç‡ä¸º0.2ï¼Œå±äºç¬¬äºŒç±»åˆ«çš„æ¦‚ç‡ä¸º0.8ï¼Œæ‰€ä»¥ä¸èƒ½åˆ©ç”¨ä¸regressionç›¸åŒçš„loss functionï¼Œéœ€è¦ç”¨è¿™ä¸ª


for t in range(100):
    out = net(x)     # å–‚ç»™ net è®­ç»ƒæ•°æ® x, è¾“å‡ºåˆ†æå€¼
    loss = loss_func(out, y)     # è®¡ç®—ä¸¤è€…çš„è¯¯å·®
    optimizer.zero_grad()   # æ¸…ç©ºä¸Šä¸€æ­¥çš„æ®‹ä½™æ›´æ–°å‚æ•°å€¼
    loss.backward()         # è¯¯å·®åå‘ä¼ æ’­, è®¡ç®—å‚æ•°æ›´æ–°å€¼
    optimizer.step()        # å°†å‚æ•°æ›´æ–°å€¼æ–½åŠ åˆ° net çš„ parameters ä¸Š

```

## ç¥ç»ç½‘ç»œçš„ä¿å­˜ä¸æå–

### save
```python
torch.save(net1, 'net.pkl')  # ä¿å­˜æ•´ä¸ªç½‘ç»œ
torch.save(net1.state_dict(), 'net_params.pkl')   # åªä¿å­˜ç½‘ç»œä¸­çš„å‚æ•° (é€Ÿåº¦å¿«, å å†…å­˜å°‘)
```

### load
```python
# æå–æ•´ä¸ªç½‘ç»œ

net2 = torch.load('net.pkl') # æå–ç½‘ç»œ
prediction = net2(x) # ä½¿ç”¨ç½‘ç»œ


# æå–ç½‘ç»œå‚æ•°

# éœ€è¦æ–°å»º net3ï¼Œå†å°†æå–çš„å‚æ•°å¯¼å…¥è¯¥ç½‘ç»œä¸­
net3 = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)

net3.load_state_dict(torch.load('net_params.pkl')) # å°†ä¿å­˜çš„å‚æ•°å¤åˆ¶åˆ° net3
prediction = net3(x) # ä½¿ç”¨ç½‘ç»œ
 
```

## Optimizer

~~æ— è„‘Adamå°±å¯ä»¥äº†~~
å‡ ç§å¸¸è§çš„ä¼˜åŒ–å™¨ï¼š`SGD`, `Momentum`, `RMSprop`, `Adam`  
ä½¿ç”¨æ–¹æ³•ï¼š
```python
# different optimizers
opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)
opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8) # æ³¨æ„momentumå°±æ˜¯SGDçš„å¥—å£³
opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)
opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))
# é¦–é€‰åä¸¤ä¸ªOptimizerï¼Œæ•ˆæœè¾ƒä½³
```