---
layout: post
title: 'Pytorchä¾¿ç­¾'
date: 2022-2-11
author: ä¸æ˜¾ç”µæ€§
cover: 'http://commcheck396.github.io/assets/img/2022_1_26/pytorch.jpg'
tags: ML Python Pytorch
---

# ~~ä¸èƒ½å†æ‹–äº†~~

> Pythorchï¼Œè½®å­åº—

[Pytorchå¸¸è§è¿ç®—](https://pytorch.org/docs/stable/torch.html)

<br/>

## Mostly Used

Numpyå‘Torchçš„è½¬åŒ–
`torch_data = torch.from_numpy(np_data)`

<br/>

Torchçš„çŸ©é˜µä¹˜æ³•
```python

torch.mm(tensor, tensor)
# 'mm' stands for 'matrix multiply'
# æ³¨æ„æ­¤å¤„ä¸Numpyçš„åŒºåˆ«ï¼Œä¸èƒ½ç›´æ¥åˆ©ç”¨.dot()è¿›è¡Œè¿ç®— 

```

<br/>  

Trochçš„æ•°å­¦è¿ç®—è§„åˆ™å‡ ä¹ä¸Numpyä¸€è‡´ï¼Œ[è¯·å‚è€ƒ](https://pytorch.org/docs/stable/torch.html)

<br/>

Torchä¸­ï¼Œè‹¥è¦è¿›è¡Œåå‘ä¼ æ’­ï¼Œéœ€è¦åˆ©ç”¨variableè¿›è¡Œè¿ç®—ï¼Œvariableå¯ä»¥ä¸€æ¬¡æ€§å°†æ‰€æœ‰ä¿®æ”¹å¹…åº¦ (æ¢¯åº¦) éƒ½è®¡ç®—å‡ºæ¥, è€Œtensorå°±æ²¡æœ‰è¿™ä¸ªèƒ½åŠ›ã€‚  
ä½†variableä¸­çš„æ•°æ®éœ€è¦tensorç±»å‹å¯¼å…¥ï¼š
```python 
tensor = torch.FloatTensor([[1,2],[3,4]])
# requires_gradæ˜¯å‚ä¸å‚ä¸è¯¯å·®åå‘ä¼ æ’­, è¦ä¸è¦è®¡ç®—æ¢¯åº¦
variable = Variable(tensor, requires_grad=True)
```

å‡ç»´
```python
x=torch.unsqueeze(x,dim=1)
```

## æ¿€åŠ±å‡½æ•°

Torch ä¸­çš„æ¿€åŠ±å‡½æ•°æœ‰å¾ˆå¤š, ä¸è¿‡æˆ‘ä»¬å¹³æ—¶è¦ç”¨åˆ°çš„å°±è¿™å‡ ä¸ªï¼š **`relu`**, `sigmoid`, `tanh`, `softplus`ã€‚ 
å¦‚ä½•åˆ©ç”¨æ¿€åŠ±å‡½æ•°:
```python
# following are popular activation functions
y_relu = torch.relu(x).numpy()
y_sigmoid = torch.sigmoid(x).numpy()
y_tanh = torch.tanh(x).numpy()
y_softplus = F.softplus(x).numpy() # there's no softplus in torch
```

## ç¥ç»ç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ

### æ™®é€šæ­å»ºæ–¹æ³•ï¼š  

```python
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output): # åˆ†åˆ«ä»£è¡¨è¾“å…¥ï¼Œè¯¥å±‚ç¥ç»å…ƒä¸ªæ•°ï¼Œè¾“å‡º
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.predict = torch.nn.Linear(n_hidden, n_output)

    def forward(self, x):
        x = F.relu(self.hidden(x)) # åˆ©ç”¨reluæ¿€åŠ±å‡½æ•°
        x = self.predict(x)
        return x

net = Net(1, 10, 1) # æ­å»ºå«æœ‰ä¸€å±‚åä¸ªhiddenç¥ç»å…ƒçš„ç¥ç»ç½‘ç»œ

# ç»“æ„ï¼š
# Net (
#   (hidden): Linear (1 -> 10)
#   (predict): Linear (10 -> 1)
# )

```

~~ğŸ¶éƒ½ä¸ç”¨ï¼Œæœ‰è½®å­è¿˜ä¸ç”¨ï¼Ÿ~~  
 <br/>

### ~~æˆ‘tmç›´æ¥~~å¿«é€Ÿæ­å»º
```python
net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(), # æ³¨æ„è¦å¤§å†™ï¼Œè¿™é‡Œçš„ReLUæ˜¯ä¸€ä¸ªclass
    torch.nn.Linear(10, 1)
)

# ç»“æ„ï¼š
# Sequential (
#   (0): Linear (1 -> 10)
#   (1): ReLU ()
#   (2): Linear (10 -> 1)
# )
```

### è®­ç»ƒæ–¹æ³•  

![pic from internet](http://commcheck396.github.io/assets/img/2022_2_14/contrast.jpg)

![pic from internet](http://commcheck396.github.io/assets/img/2022_2_14/example.png)
#### å›å½’æ‹Ÿåˆ
```python
# optimizer æ˜¯è®­ç»ƒçš„å·¥å…·
optimizer = torch.optim.SGD(net.parameters(), lr=0.2)  # ä¼ å…¥ net çš„æ‰€æœ‰å‚æ•°, learning rate
loss_func = torch.nn.MSELoss()      # é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„è¯¯å·®è®¡ç®—å…¬å¼ (å‡æ–¹å·®)

for t in range(100):
    prediction = net(x)     # å–‚ç»™ net è®­ç»ƒæ•°æ® x, è¾“å‡ºé¢„æµ‹å€¼
    loss = loss_func(prediction, y)     # è®¡ç®—ä¸¤è€…çš„è¯¯å·®,predictionè¦åœ¨å‰
    optimizer.zero_grad()   # æ¸…ç©ºä¸Šä¸€æ­¥çš„æ®‹ä½™æ›´æ–°å‚æ•°å€¼
    loss.backward()         # è¯¯å·®åå‘ä¼ æ’­, è®¡ç®—å‚æ•°æ›´æ–°å€¼
    optimizer.step()        # å°†å‚æ•°æ›´æ–°å€¼æ–½åŠ åˆ° net çš„ parameters ä¸Š
```

#### åŒºåˆ†ç±»å‹
åˆ©ç”¨one-hot vectorè¾“å‡ºï¼Œå¹¶åˆ©ç”¨softmaxè¿›è¡Œnormalizationï¼Œä½†åœ¨Pytorchä¸­softmaxå·²ç»è¢«æ•´åˆåœ¨CrossEntropyä¸­ï¼Œæ•…ä¸å¿…å•ç‹¬è¿›è¡Œsoftmaxæ“ä½œ
![pic from internet](http://commcheck396.github.io/assets/img/2022_2_14/ont-hot.png)
```python
net = Net(n_feature=2, n_hidden=10, n_output=2) # å‡ ä¸ªç±»åˆ«å°±å‡ ä¸ª output

# optimizer æ˜¯è®­ç»ƒçš„å·¥å…·
optimizer = torch.optim.SGD(net.parameters(), lr=0.02)  # ä¼ å…¥ net çš„æ‰€æœ‰å‚æ•°,learning rate
loss_func = torch.nn.CrossEntropyLoss() 
# æ³¨æ„loss functionçš„æ”¹å˜ï¼Œæ­¤æ—¶çš„è¾“å‡ºæ˜¯äºŒç»´æ•°æ®ï¼Œæ¯”å¦‚[0.2,0.8]ï¼Œä»£è¡¨è¯¥ç‚¹å±äºç¬¬ä¸€ç±»åˆ«çš„æ¦‚ç‡ä¸º0.2ï¼Œå±äºç¬¬äºŒç±»åˆ«çš„æ¦‚ç‡ä¸º0.8ï¼Œæ‰€ä»¥ä¸èƒ½åˆ©ç”¨ä¸regressionç›¸åŒçš„loss functionï¼Œéœ€è¦ç”¨è¿™ä¸ª


for t in range(100):
    out = net(x)     # å–‚ç»™ net è®­ç»ƒæ•°æ® x, è¾“å‡ºåˆ†æå€¼
    loss = loss_func(out, y)     # è®¡ç®—ä¸¤è€…çš„è¯¯å·®
    optimizer.zero_grad()   # æ¸…ç©ºä¸Šä¸€æ­¥çš„æ®‹ä½™æ›´æ–°å‚æ•°å€¼
    loss.backward()         # è¯¯å·®åå‘ä¼ æ’­, è®¡ç®—å‚æ•°æ›´æ–°å€¼
    optimizer.step()        # å°†å‚æ•°æ›´æ–°å€¼æ–½åŠ åˆ° net çš„ parameters ä¸Š

```

## ç¥ç»ç½‘ç»œçš„ä¿å­˜ä¸æå–

### save
```python
torch.save(net1, 'net.pkl')  # ä¿å­˜æ•´ä¸ªç½‘ç»œ
torch.save(net1.state_dict(), 'net_params.pkl')   # åªä¿å­˜ç½‘ç»œä¸­çš„å‚æ•° (é€Ÿåº¦å¿«, å å†…å­˜å°‘)
```

### load
```python
# æå–æ•´ä¸ªç½‘ç»œ

net2 = torch.load('net.pkl') # æå–ç½‘ç»œ
prediction = net2(x) # ä½¿ç”¨ç½‘ç»œ


# æå–ç½‘ç»œå‚æ•°

# éœ€è¦æ–°å»º net3ï¼Œå†å°†æå–çš„å‚æ•°å¯¼å…¥è¯¥ç½‘ç»œä¸­
net3 = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)

net3.load_state_dict(torch.load('net_params.pkl')) # å°†ä¿å­˜çš„å‚æ•°å¤åˆ¶åˆ° net3
prediction = net3(x) # ä½¿ç”¨ç½‘ç»œ
 
```

## Optimizer

~~æ— è„‘Adamå°±å¯ä»¥äº†~~
![pic from internet](http://commcheck396.github.io/assets/img/2022_2_14/adam.png)
å‡ ç§å¸¸è§çš„ä¼˜åŒ–å™¨ï¼š`SGD`, `Momentum`, `RMSprop`, `Adam`  
ä½¿ç”¨æ–¹æ³•ï¼š
```python
# different optimizers
opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)
opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8) # æ³¨æ„momentumå°±æ˜¯SGDçš„å¥—å£³
opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)
opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))
# é¦–é€‰åä¸¤ä¸ªOptimizerï¼Œæ•ˆæœè¾ƒä½³
```

## åˆ©ç”¨Pytorchæ­å»ºCNNç½‘ç»œ

ä»¥MNISTä»»åŠ¡ä¸ºä¾‹
```python 
# å¼•å…¥å¦‚ä¸‹library
import torch.nn as nn
import torch.utils.data as Data
import torchvision

# ç»™å®šHyper Parameters
EPOCH = 1               # train the training data n times, to save time, we just train 1 epoch
BATCH_SIZE = 50
LR = 0.001              # learning rate
```
### æ•°æ®çš„è®­ç»ƒä¸æµ‹è¯•
```python
# è®­ç»ƒsetçš„åˆå§‹åŒ–
train_data = torchvision.datasets.MNIST(
    root='./mnist/',
    train=True,                                     # this is training data
    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to
                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]
    download=DOWNLOAD_MNIST,                        # downloaded or not
)
train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True) # åŠ è½½training set


# pick 2000 samples to speed up testing
test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)
test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255.   # ç”±äºä»…ä»…å¯¹train_dataè¿›è¡Œäº†tensorå¤„ç†ï¼ŒåŒæ ·ä¹Ÿéœ€è¦å¯¹test_dataè¿›è¡Œå¤„ç†ï¼Œä½¿å…¶å–å€¼èŒƒå›´åœ¨[0,1]ï¼Œæ‰€ä»¥å¯¹å…¶é™¤255
test_y = test_data.test_labels[:2000]  # ä»…å–å‰2000ä¸ªè¿›è¡Œtest

```

### æ­å»ºCNNç½‘ç»œ
CNNåŸºç¡€æ¶æ„å¦‚å›¾æ‰€ç¤º
![pic from internet](http://commcheck396.github.io/assets/img/2022_2_14/CNN.png)
![pic from internet](http://commcheck396.github.io/assets/img/2022_2_14/filter.jpg)
paddingçš„æ„ä¹‰ï¼šä¸ºäº†ä½¿ç»è¿‡convçš„å›¾ç‰‡å¤§å°ä¸è¢«å‹ç¼©ï¼Œè®¾ä¸ºâ€˜sameâ€™å³å¯
![pic from internet](http://commcheck396.github.io/assets/img/2022_2_14/padding.gif)
Pytorchå®ç°å¦‚ä¸‹

```python
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(  # input shape (1, 28, 28)
            nn.Conv2d(
                in_channels=1,      # input height
                out_channels=16,    # filterä¸ªæ•°
                kernel_size=5,      # filter size
                stride=1,           # filter movement/stepï¼Œæ­¥é•¿
                padding=2,      # å¦‚æœæƒ³è¦ con2d å‡ºæ¥çš„å›¾ç‰‡é•¿å®½æ²¡æœ‰å˜åŒ–, padding=(kernel_size-1)/2 å½“ stride=1  
            ),      # output shape (16, 28, 28)
            nn.ReLU(),    # activation
            nn.MaxPool2d(kernel_size=2),    # åœ¨ 2x2 ç©ºé—´é‡Œå‘ä¸‹é‡‡æ ·, output shape (16, 14, 14)ï¼Œç”±äºç»å†äº†ä¸€ä¸ªkerne_sizeä¸º2çš„poolingï¼Œé•¿å®½å˜ä¸ºä¹‹å‰çš„ä¸€åŠï¼Œç»è¿‡16ä¸ªfilterï¼Œå˜ä¸º16
        )
        self.conv2 = nn.Sequential(  # input shape (16, 14, 14)
            nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)
            nn.ReLU(),  # activation
            nn.MaxPool2d(2),  # å†è¿›è¡Œä¸€æ¬¡poolingï¼Œoutput shape (32, 7, 7)
        )
        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0), -1)   # å±•å¹³å¤šç»´çš„å·ç§¯å›¾æˆ (batch_size, 32 * 7 * 7)  
        output = self.out(x)
        return output

cnn = CNN()  # åˆå§‹åŒ–ç¥ç»ç½‘ç»œ
```

### è®­ç»ƒè¿‡ç¨‹
```python 
optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters
loss_func = nn.CrossEntropyLoss()   # the target label is not one-hotted

# training and testing
for epoch in range(EPOCH):
    for step, (b_x, b_y) in enumerate(train_loader):   # åˆ†é… batch data, normalize x when iterate train_loader
        output = cnn(b_x)               # cnn output
        loss = loss_func(output, b_y)   # cross entropy loss
        optimizer.zero_grad()           # clear gradients for this training step
        loss.backward()                 # backpropagation, compute gradients
        optimizer.step()                # apply gradients
```

### è®­ç»ƒç»“æŸåé¢„æµ‹åä¸ªæ•°å­—çœ‹ä¸€ä¸‹
```python
test_output = cnn(test_x[:10])
pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()
print(pred_y, 'prediction number')
print(test_y[:10].numpy(), 'real number')
```
ç»“æœä¸€èˆ¬éƒ½ä¼šå¾ˆå¯¹ï¼Œæ¯•ç«ŸMINSTä¹Ÿæ²¡ä»€ä¹ˆéš¾åº¦  
~~ä½†æ˜¯ä¸€å°å¥½çš„ç”µè„‘çœŸçš„å¾ˆé‡è¦~~
