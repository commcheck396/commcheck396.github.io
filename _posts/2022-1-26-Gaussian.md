---
layout: post
title: 'äº¤æµé¡¹ç›®æ€»ç»“_Gaussian Processes'
date: 2022-1-26
author: ä¸æ˜¾ç”µæ€§
cover: 'http://commcheck396.github.io/assets/img/2022_1_25/topic.png'
tags: ML Python
---

> ~~ç»å…¸ç™½å¿™æ´»~~

<br/>
çº¿æ€§å›å½’æ¨¡å‹çš„å‡è®¾ç©ºé—´ç›¸å½“æœ‰é™ï¼Œä¸ºäº†è®©æˆ‘ä»¬çš„æ¨¡å‹æ›´åŠ é€‚åº”æ™®éçš„æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥æ›´åŠ ä¸°å¯Œå¤šå˜çš„å‡è®¾ï¼Œå³é«˜æ–¯è¿‡ç¨‹ã€‚

## Kernel Regression
>In order to work with a GP it makes sense to write a bit more sensible structured code and break up a few things into functions. The first thing we want to implement is a function that allows us to compute the covariance matrix. It could be nice to try and modularise this so that you can easily use your code with several different covariance functions.

ä¸ºäº†å®ç°GPåŠŸèƒ½ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åšçš„äº‹æƒ…æ˜¯è®¡ç®—å‡ºæ ·æœ¬çš„åæ–¹å·®çŸ©é˜µï¼Œè€Œè·å¾—è¿™ä¸ªçŸ©é˜µçš„æœ€ä½³æ–¹æ³•ï¼Œä¾¿æ˜¯åˆ©ç”¨Kernel Regressionï¼Œä¸‹æ–¹å°±æ˜¯åŸºäºKernel Regressionç¼–å†™çš„æ±‚åæ–¹å·®çš„å‡½æ•°ï¼ŒvarSigmaæ§åˆ¶å‡½æ•°å‚ç›´æ–¹å‘çš„æ³¢åŠ¨æ€§ï¼Œlengthscaleæ§åˆ¶å‡½æ•°çš„å¹³æ»‘åº¦ï¼Œè¿™ä¸ªå‡½æ•°å¯ä»¥æ±‚å¾—ç‚¹é›†x1å’Œx2çš„åæ–¹å·®ï¼š
```python 
def rbf_kernel(x1, x2, varSigma, lengthscale):
    if x2 is None:
        d = cdist(x1, x1)
    else:
        d = cdist(x1, x2)
    K = varSigma*np.exp(-np.power(d, 2)/lengthscale)
    return K

#    Args:
#        X1: ndArrayï¼Œ mä¸ªç‚¹ (m x d).
#        X2: ndArrayï¼Œ nä¸ªç‚¹ (n x d).
#    è¿”å›:
#        åæ–¹å·®çŸ©é˜µ (m x n).

```
æˆ‘ä»¬ä¹Ÿå¯ä»¥é’ˆå¯¹è¿™ä¸ªå‡½æ•°ï¼Œèµ‹ä¸€äº›æ¯”è¾ƒç®€å•çš„å€¼ï¼Œè¿›è¡Œä¸€äº›testï¼š
```python
x = np.linspace(-5, 5, 200).reshape(-1, 1)
c=x.shape
# compute covariance matrix
K = rbf_kernel(x, None, 1.0, 2.0)
# create mean vector
mu = np.zeros(c[0])
# draw samples 20 from Gaussian distribution
f = np.random.multivariate_normal(mu, K, 20)
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(x, f.T)
plt.show()
```
#### å½“`varSigma=1.0`,`lengthscale=2.0`æ—¶ï¼š
å¾—å‡ºå¦‚ä¸‹çš„å‡½æ•°æ›²çº¿ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªæ›²çº¿ä½œä¸ºåŸºå‡†å€¼è¿›è¡Œå¯¹æ¯”ï¼š

![pic from internet](http://commcheck396.github.io/assets/img/2022_1_26/1,2.png)

#### å½“`varSigma=1.0`,`lengthscale=50.0`æ—¶ï¼š
å¾—å‡ºå¦‚ä¸‹çš„å‡½æ•°æ›²çº¿ï¼š

![pic from internet](http://commcheck396.github.io/assets/img/2022_1_26/1,50.png)

ä¸éš¾çœ‹å‡ºï¼Œåœ¨`lengthscale`å‚æ•°è¾ƒå¤§æ—¶ï¼Œå‡½æ•°çš„å¹³æ»‘åº¦ç›¸è¾ƒäºç¬¬ä¸€å¹…å‡½æ•°å›¾åƒæœ‰äº†å¾ˆå¤§çš„æå‡ã€‚

#### å½“`varSigma=10.0`,`lengthscale=2.0`æ—¶ï¼š
å¾—å‡ºå¦‚ä¸‹å‡½æ•°æ›²çº¿ï¼š

![pic from internet](http://commcheck396.github.io/assets/img/2022_1_26/10,2.png)

ä¸éš¾çœ‹å‡ºï¼Œåœ¨`varSigma`å‚æ•°è¾ƒå¤§æ—¶ï¼Œå‡½æ•°çš„çºµåæ ‡æ€»ä½“éƒ½æœ‰äº†ä¸€å®šç¨‹åº¦çš„å¢å¤§ï¼Œè¯´æ˜å‡½æ•°åœ¨å‚ç›´æ–¹å‘çš„æ³¢åŠ¨æ€§æå‡äº†ã€‚

<br/>

>The important thing to understand is that even if the process is infinite, as are the number of values the function can be evaluated across, due to the consistency of a GP we can decide to only look at a finite subset of the process. This is what we define using x. Test to increase and decrease the cardinality of the index set. Sampling from the prior is very important as it allows us to see what assumptions we can encode with the parameters. 
Importantly, the GP places non-zero probability on every function so if you sample for long enough everything will appear, with a few samples you are only seeing the most likely things.


ç”±äºGPåœ¨æ¯ä¸ªå‡½æ•°ä¸Šéƒ½æ”¾ç½®äº†ä¸€ä¸ªéé›¶çš„æ¦‚ç‡ï¼Œæ‰€ä»¥ï¼Œåœ¨æ—¶é—´å…è®¸çš„æƒ…å†µä¸‹ï¼Œä»»ä½•å½¢çŠ¶çš„å‡½æ•°éƒ½å¯ä»¥è¢«é‡‡æ ·åˆ°ã€‚


<br/><br/><br/>

> Now lets try to change the covariance function and see how the samples change, lets implement three more covariance functions, a white-noise, a linear and a periodic covariance.

æ¥ä¸‹æ¥è¿˜æœ‰ä¸‰ä¸ªå¦å¤–çš„åæ–¹å·®å‡½æ•°ï¼Œæˆ‘ä»¬ä¸€æ¬¡å¯¹ä»–ä»¬è¿›è¡Œæµ‹è¯•ï¼š 

**æµ‹è¯•å‚æ•°ï¼š`varSigma=1.0`,`lengthscale=2.0`,`period=2`**

### white-noise covariance
```python
def white_kernel(x1, x2, varSigma):
    if x2 is None:
        return varSigma*np.eye(x1.shape[0])
    else:
        return np.zeros(x1.shape[0], x2.shape[0])
```
å‡½æ•°å›¾åƒå¦‚ä¸‹ï¼š

![pic from internet](http://commcheck396.github.io/assets/img/2022_1_26/w.png)


### linear covariance
```python
def lin_kernel(x1, x2, varSigma):
    if x2 is None:
        return varSigma*x1.dot(x1.T)
    else:
        return varSigma*x1.dot(x2.T)
```
å‡½æ•°å›¾åƒå¦‚ä¸‹ï¼š

![pic from internet](http://commcheck396.github.io/assets/img/2022_1_26/l.png)


### periodic covariance
```python
def periodic_kernel(x1, x2, varSigma, period, lenthscale):
    if x2 is None:
        d = cdist(x1, x1)
    else:
        d = cdist(x1, x2)
    return varSigma*np.exp(-(2*np.sin((np.pi/period)*d)**2)/lenthscale**2)
```
å‡½æ•°å›¾åƒå¦‚ä¸‹ï¼š

![pic from internet](http://commcheck396.github.io/assets/img/2022_1_26/p.png)

é€šè¿‡å¯¹è¿™äº›å›¾åƒçš„è§‚å¯Ÿï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä¸åŒçš„åæ–¹å·®å‡½æ•°ç”Ÿæˆçš„å¯¹åº”å‡½æ•°å›¾åƒçš„ç‰¹ç‚¹ã€‚  
ä¸ä»…å¦‚æ­¤ï¼Œæˆ‘ä»¬é™¤äº†å¯ä»¥è§‚å¯Ÿä¸åŒå‡½æ•°å›¾åƒçš„ç‰¹ç‚¹å¤–ï¼Œè¿˜å¯ä»¥å°†ä¸åŒçš„åæ–¹å·®ç»“æœç›¸ä¹˜ï¼Œè§‚å¯Ÿå‡½æ•°å›¾å½¢çš„ç‰¹ç‚¹ï¼š

### periodic covariance * linear covariance
å‡½æ•°å›¾åƒå¦‚ä¸‹ï¼š

![pic from internet](http://commcheck396.github.io/assets/img/2022_1_26/pxl.png)

é€šè¿‡è§‚å¯Ÿæˆ‘ä»¬ä¸éš¾å‘ç°ï¼Œè¿™ä¸ªå›¾åƒç»¼åˆäº†ä¸¤ä¸ªåæ–¹å·®å›¾åƒçš„ç‰¹ç‚¹ï¼Œæ—¢å…·æœ‰çº¿æ€§ï¼Œåœ¨æŸæ–¹å‘ä¹Ÿå…·æœ‰ä¸€äº›å‘¨æœŸæ€§ï¼Œso funnyã€‚

### periodic covariance * white-noise covariance
å‡½æ•°å›¾åƒå¦‚ä¸‹ï¼š

![pic from internet](http://commcheck396.github.io/assets/img/2022_1_26/pxw.png)

å“ˆå“ˆå“ˆå“ˆå“ˆï¼Œç”±äºç™½å™ªéŸ³æœ¬èº«å°±æ˜¯æ‚ä¹±æ— ç« çš„ï¼Œè¿™å¹…å›¾åƒä¼¼ä¹ä¸white-noise covarianceå•ç‹¬ä½œç”¨çš„åŒºåˆ«å¹¶ä¸å¤§ï¼Œæ—¢ç„¶white-noise covarianceçš„å½±å“å¦‚æ­¤ä¹‹å¤§ï¼Œä¸è®©è®©å®ƒå’Œlinear covarianceç»„åˆä¸€ä¸‹ï¼Œçœ‹çœ‹ä¼šäº§ç”Ÿä»€ä¹ˆç»“æœï¼Œæ˜¯å¦è¿˜ä¼šåƒè¿™æ¬¡ä¸€æ ·ç›´æ¥è¢«white-noise covarianceåŒåŒ–ã€‚

### linear covariance * white-noise covariance
å‡½æ•°å›¾åƒå¦‚ä¸‹ï¼š

![pic from internet](http://commcheck396.github.io/assets/img/2022_1_26/lxw.png)

å“ˆå“ˆï¼ŒçŒœä¸­äº†ï¼Œä½†æ²¡å®Œå…¨çŒœä¸­ï¼Œè¿™å¹…å›¾åƒæ—¢ç»§æ‰¿äº†white-noise covarianceçš„æ‚ä¹±æ€§ï¼Œä¹Ÿä¿ç•™äº†linear covarianceçš„çº¿æ€§ï¼Œç”±äºlinear covarianceçš„çº¿æ€§ï¼Œæ•´å¹…å›¾åƒæ‰å±•ç°å‡ºäº†è¿™æ ·çš„æ‚ä¹±è¿™å‘ä¸¤ä¾§å‘æ•£çš„å›¾å½¢ã€‚  
åšä¸€ä¸ªå°å°çš„é¢„æµ‹ï¼Œå¦‚æœæˆ‘å°†è¿™ä¸‰ä¸ªç»“æœä¹˜åœ¨ä¸€èµ·ï¼Œå›¾åƒå½¢çŠ¶åº”è¯¥ä¸è¿™å¹…å›¾ç±»ä¼¼ï¼Œçºµåæ ‡ç»å¯¹å€¼ç¨å¤§ï¼Œå› ä¸ºperiodic covarianceçš„å‘¨æœŸæ€§å®Œå…¨è¢«white-noise covarianceæŠµæ¶ˆäº†ã€‚

### linear covariance * white-noise covariance * periodic covariance
å‡½æ•°å›¾åƒå¦‚ä¸‹ï¼š

![pic from internet](http://commcheck396.github.io/assets/img/2022_1_26/lxwxp.png)

å˜¿å˜¿ï¼Œå¯¹å–½ã€‚

## å›°äº†ï¼Œæ˜å¤©å†è¯´ğŸ˜´