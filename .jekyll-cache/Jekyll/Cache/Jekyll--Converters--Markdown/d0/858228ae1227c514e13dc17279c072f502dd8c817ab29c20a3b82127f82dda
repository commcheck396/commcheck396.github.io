I"Ú<h1 id="attention-is-all-u-need">Attention is all U need</h1>

<p>åœ¨äº†è§£transformeræ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆè¦ææ¸…self-attentionçš„æ¦‚å¿µã€‚ 
Self-attentionï¼Œè¾“å…¥æ˜¯ä¸€ä¸²vector setï¼Œè¾“å‡ºäº¦ç„¶ï¼ŒRNNç½‘ç»œåŒæ ·å¯ä»¥å®ç°ç±»ä¼¼çš„äº‹æƒ…è€Œä¸”æ›´å¥½æ­å»ºï¼Œä½†æ˜¯Self-attentionå¯ä»¥å®ç°æ•°æ®çš„å¹¶è¡Œå¤„ç†ï¼Œè€ŒRNNä»…å¯ä»¥å®ç°ä¸²è¡Œï¼Œæ‰€ä»¥ä¼˜å…ˆç ”ç©¶è¿™ä¸ªæ•ˆç‡è¾ƒé«˜çš„æ–¹å‘äº†ï¼Œä¹Ÿå¯èƒ½ä¼šå»å­¦ä¸€ä¸‹RNNï¼Œ<del>å› ä¸ºè¿™ä¸ªæ­å»ºèµ·æ¥å®åœ¨æ˜¯å¤ªéº»çƒ¦äº†</del>ï¼Œæ”¾åœ¨Pytorchä¾¿ç­¾ä¸­å§ã€‚</p>

<p>Self-attentionå…¶å®ä¸éš¾ç†è§£ï¼Œç®€è€Œè¨€ä¹‹å°±æ˜¯ç”¨å„ç§æ–¹æ³•åœ¨è¾“å…¥çš„å‘é‡é—´æ‰¾å½¼æ­¤çš„å…³ç³»Î±ï¼Œç„¶åå¯¹è¾“å…¥å†…å®¹è¿›è¡Œé¢„æµ‹ï¼Œè¾“å‡ºä¸€ä¸ªvector setã€‚ç›´æ¥ä¸Šå›¾ã€‚
<img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/selfattention.jpg" alt="pic from internet" />
è¿™æ˜¯self-attentionçš„æ•´ä¸ªæµç¨‹ï¼Œå¹¶éç¥ç»ç½‘ç»œï¼è‹¥è¦è¿›è¡Œæœºå™¨å­¦ä¹ è®­ç»ƒï¼Œè¿˜éœ€è¦æ­å»ºç¥ç»ç½‘ç»œï¼Œè¿™ä¹Ÿä¾¿æœ‰äº†transformeræ¨¡å‹ã€‚<br />
åœ¨åŸå§‹è®ºæ–‡ä¸­Self-Attentionä¸­æ²¡æœ‰è€ƒè™‘ä½ç½®ä¿¡æ¯ï¼Œä¸å¦¨åŠ ä¸€ä¸ªeiæ¥è¡¨ç¤ºä½ç½®ä¿¡æ¯ï¼Œæ€ä¹ˆç†è§£å‘¢ï¼Œå¯ä»¥ç†è§£ä¸ºåœ¨xiå‘é‡ä¸ŠåŠ äº†ä¸€ä¸ªone-hotè¡¨ç¤ºçš„piï¼Œç„¶åç»è¿‡è®¡ç®—å‘ç°eiå¹¶ä¸å½±å“åŸæ¥çš„å‘é‡ï¼Œæ‰€ä»¥åŠ å…¥è¿™ä¸ªä½ç½®ä¿¡æ¯ä¸ä»…ä¸ä¼šå½±å“å·²æœ‰çš„æ•°æ®ï¼Œè¿˜èƒ½åœ¨è¾“å…¥ä¸­åŠ å…¥æœ‰å…³ä½ç½®çš„ä¿¡æ¯ï¼Œå¯è°“ä¸€ä¸¾ä¸¤å¾—ã€‚
<img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/position.png" alt="pic from internet" /></p>

<p>å…¶å®ï¼Œtransformeræ¨¡å‹å’Œä¸Šè¿°è¿‡ç¨‹å¹¶éå®Œå…¨ç›¸å…³ï¼Œä¸ä¹‹æ›´ä¸ºç›¸å…³çš„æ˜¯ä¸‹æ–¹çš„multihead
<img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/multi.jpg" alt="pic from internet" />
çœ‹è¿‡äº†æ•´ä¸ªè·¯ç¨‹ï¼Œä¸éš¾å‘ç°æˆ‘ä»¬éœ€è¦å­¦ä¹ çš„å‚æ•°ä¸€å…±å°±ä¸‹é¢å‡ ä¸ªå„¿
<img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/parameter.jpg" alt="pic from internet" />
Self-attentionä¹Ÿå°±è¿™ä¹ˆå¤šï¼Œä¸‹é¢è¿›å…¥æ­£é¢˜transformerã€‚</p>

<h2 id="transformerå®ç°">Transformerå®ç°</h2>
<p>è¿™ä¸ªæ¨¡å‹å¯ä»¥çœ‹æˆæ˜¯ä¸€ä¸ªé»‘ç®±æ“ä½œã€‚åœ¨æœºå™¨ç¿»è¯‘ä¸­ï¼Œå°±æ˜¯è¾“å…¥ä¸€ç§è¯­è¨€ï¼Œè¾“å‡ºå¦ä¸€ç§è¯­è¨€ã€‚
<img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/transformer.png" alt="pic from internet" />
è¿™ä¸ªé»‘ç®±æ˜¯ç”±ç¼–ç ç»„ä»¶ã€è§£ç ç»„ä»¶å’Œå®ƒä»¬ä¹‹é—´çš„è¿æ¥ç»„æˆã€‚</p>

<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/blackbox.png" alt="pic from internet" /></p>

<p>ç¼–ç ç»„ä»¶éƒ¨åˆ†ç”±ä¸€å †ç¼–ç å™¨ï¼ˆencoderï¼‰æ„æˆï¼ˆè®ºæ–‡ä¸­æ˜¯å°†6ä¸ªç¼–ç å™¨å åœ¨ä¸€èµ·ï¼‰ã€‚è§£ç ç»„ä»¶éƒ¨åˆ†ä¹Ÿæ˜¯ç”±ç›¸åŒæ•°é‡ï¼ˆä¸ç¼–ç å™¨å¯¹åº”ï¼‰çš„è§£ç å™¨ï¼ˆdecoderï¼‰ç»„æˆçš„ã€‚æ‰€æœ‰çš„ç¼–ç å™¨åœ¨ç»“æ„ä¸Šéƒ½æ˜¯ç›¸åŒçš„ï¼Œä½†å®ƒä»¬æ²¡æœ‰å…±äº«å‚æ•°ã€‚æ¯ä¸ªè§£ç å™¨éƒ½å¯ä»¥åˆ†è§£æˆä¸¤ä¸ªå­å±‚ã€‚</p>

<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/bianmaqi.png" alt="pic from internet" /></p>

<p>ä»ç¼–ç å™¨è¾“å…¥çš„å¥å­é¦–å…ˆä¼šç»è¿‡ä¸€ä¸ªä¸Šæ–‡æåˆ°çš„è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰å±‚ï¼Œè¿™å±‚å¸®åŠ©ç¼–ç å™¨åœ¨å¯¹æ¯ä¸ªå•è¯ç¼–ç æ—¶å…³æ³¨è¾“å…¥å¥å­çš„å…¶ä»–å•è¯ã€‚</p>

<p>è‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡ºä¼šä¼ é€’åˆ°å‰é¦ˆï¼ˆfeed-forwardï¼‰ç¥ç»ç½‘ç»œä¸­ã€‚æ¯ä¸ªä½ç½®çš„å•è¯å¯¹åº”çš„å‰é¦ˆç¥ç»ç½‘ç»œéƒ½å®Œå…¨ä¸€æ ·ã€‚</p>

<p>è§£ç å™¨ä¸­ä¹Ÿæœ‰ç¼–ç å™¨çš„è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰å±‚å’Œå‰é¦ˆï¼ˆfeed-forwardï¼‰å±‚ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿™ä¸¤ä¸ªå±‚ä¹‹é—´è¿˜æœ‰ä¸€ä¸ªæ³¨æ„åŠ›å±‚ï¼Œç”¨æ¥å…³æ³¨è¾“å…¥å¥å­çš„ç›¸å…³éƒ¨åˆ†ï¼ˆå’Œseq2seqæ¨¡å‹çš„æ³¨æ„åŠ›ä½œç”¨ç›¸ä¼¼ï¼‰ã€‚</p>

<p>Transformer çš„ Decoderçš„è¾“å…¥ä¸Encoderçš„è¾“å‡ºå¤„ç†æ–¹æ³•æ­¥éª¤æ˜¯ä¸€æ ·åœ°ï¼Œä¸€ä¸ªæ¥å—sourceæ•°æ®ï¼Œä¸€ä¸ªæ¥å—targetæ•°æ®ï¼Œä¸¾ä¸ªä¾‹å­ï¼šEncoderæ¥å—è‹±æ–‡â€Tom chase Jerryâ€ï¼ŒDecoderæ¥å—ä¸­æ–‡â€æ±¤å§†è¿½é€æ°ç‘â€ã€‚åªæ˜¯åœ¨æœ‰targetæ•°æ®æ—¶ä¹Ÿå°±æ˜¯åœ¨è¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒæ—¶æ‰ä¼šæ¥å—Outputs Embeddingï¼Œè¿›è¡Œé¢„æµ‹æ—¶åˆ™ä¸ä¼šæ¥æ”¶ã€‚</p>

<p>ä¹‹åå°±è¦å¼•å…¥æˆ‘ä»¬çš„å¼ é‡äº†ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ¯ä¸ªè¾“å…¥å•è¯é€šè¿‡è¯åµŒå…¥ç®—æ³•è½¬æ¢ä¸ºè¯å‘é‡ï¼Œæ¯ä¸ªå•è¯éƒ½è¢«åµŒå…¥ä¸º512ç»´çš„å‘é‡ã€‚</p>

<p>è¯åµŒå…¥è¿‡ç¨‹åªå‘ç”Ÿåœ¨æœ€åº•å±‚çš„ç¼–ç å™¨ä¸­ã€‚æ‰€æœ‰çš„ç¼–ç å™¨éƒ½æœ‰ä¸€ä¸ªç›¸åŒçš„ç‰¹ç‚¹ï¼Œå³å®ƒä»¬æ¥æ”¶ä¸€ä¸ªå‘é‡åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸ªå‘é‡å¤§å°ä¸º512ç»´ã€‚åœ¨åº•å±‚ï¼ˆæœ€å¼€å§‹ï¼‰ç¼–ç å™¨ä¸­å®ƒå°±æ˜¯è¯å‘é‡ï¼Œä½†æ˜¯åœ¨å…¶ä»–ç¼–ç å™¨ä¸­ï¼Œå®ƒå°±æ˜¯ä¸‹ä¸€å±‚ç¼–ç å™¨çš„è¾“å‡ºï¼ˆä¹Ÿæ˜¯ä¸€ä¸ªå‘é‡åˆ—è¡¨ï¼‰ã€‚å‘é‡åˆ—è¡¨å¤§å°æ˜¯æˆ‘ä»¬å¯ä»¥è®¾ç½®çš„è¶…å‚æ•°â€”â€”ä¸€èˆ¬æ˜¯æˆ‘ä»¬è®­ç»ƒé›†ä¸­æœ€é•¿å¥å­çš„é•¿åº¦ã€‚</p>

<p>æˆ‘ä»¬è¿˜éœ€è¦ç»™æ¯ä¸ªwordçš„è¯å‘é‡æ·»åŠ ä½ç½®ç¼–ç positional encodingï¼Œä¸ºä»€ä¹ˆéœ€è¦æ·»åŠ ä½ç½®ç¼–ç å‘¢ï¼Ÿ</p>

<p>é¦–å…ˆå’±ä»¬çŸ¥é“ï¼Œä¸€å¥è¯ä¸­åŒä¸€ä¸ªè¯ï¼Œå¦‚æœè¯è¯­å‡ºç°ä½ç½®ä¸åŒï¼Œæ„æ€å¯èƒ½å‘ç”Ÿç¿»å¤©è¦†åœ°çš„å˜åŒ–ï¼Œå°±æ¯”å¦‚ï¼šæˆ‘æ¬ ä»–100W å’Œ ä»–æ¬ æˆ‘100Wã€‚è¿™ä¸¤å¥è¯çš„æ„æ€ä¸€ä¸ªåœ°ç‹±ä¸€ä¸ªå¤©å ‚ã€‚å¯è§è·å–è¯è¯­å‡ºç°åœ¨å¥å­ä¸­çš„ä½ç½®ä¿¡æ¯æ˜¯ä¸€ä»¶å¾ˆé‡è¦çš„äº‹æƒ…ã€‚</p>

<p>è¿™positional encodingçš„è·å–ä¹Ÿæ˜¯ä¸€é—¨å­¦é—®ï¼Œä¸€èˆ¬æˆ‘ä»¬ä¼šç”¨ä¸‹é¢ä¸¤ä¸ªå…¬å¼æ¥è·å–ã€‚</p>

<p>å•¥ï¼Ÿä½ é—®ä¸ºå•¥ï¼Ÿåˆ«é—®ï¼Œé—®å°±æ˜¯å¤åœ£å…ˆè´¤ã€‚</p>

<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/positionf.png" alt="pic from internet" /></p>

<h3 id="encoder">encoder</h3>

<p>self-attentionç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>

<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/encoderlayer.png" alt="pic from internet" /></p>

<p>ä½†åœ¨encoder layerä¸­è¿ç”¨çš„æ¶æ„å¹¶éè¿™ä¸€ä¸ªï¼Œè€Œæ˜¯Multi-Head Attentionï¼Œè¿™ä¸ªé—®é¢˜åœ¨ä¸Šæ–‡ä¹Ÿæœ‰è®¨è®ºè¿‡ï¼Œå…¶å®å®ƒå°±æ˜¯åœ¨self-attentionçš„åŸºç¡€ä¸Šï¼Œå¯¹äºè¾“å…¥çš„embeddingçŸ©é˜µæœ‰å¤šä¸ªçŸ©é˜µè¿›è¡Œæ•°æ®çš„å¤„ç†ï¼Œå¹¶åœ¨å¾—åˆ°å¤šä¸ªç»“æœåå†è¿›è¡Œé™ç»´ï¼Œå¾—åˆ°æœ€ç»ˆç»“æœã€‚</p>

<p>è€Œè¿™ä¸ªé™ç»´æ“ä½œï¼Œå±•å¼€æ¥è¯´å°±æ˜¯<strong>Addï¼†Normalize</strong></p>

<p>ç®€å•æ¥è¯´ï¼ŒAddæ“ä½œçš„ä½œç”¨å°±æ˜¯åœ¨è¾“å…¥ä¸­åŠ å…¥æ®‹å·®å—ï¼Œé˜²æ­¢ç¥ç»ç½‘ç»œç”±äºlayerè¿‡å¤šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿé€€åŒ–é—®é¢˜ï¼Œè€Œè¿™ä¸ªæ®‹å·®å—æ¶‰åŠåˆ°resnetæ–¹é¢çš„çŸ¥è¯†ï¼Œå¯¹è¿™æ–¹é¢æˆ‘äº†è§£ç”šå°‘ï¼Œæ‰€ä»¥å°±å…ˆä¸æ±‚ç”šè§£ä¸€ä¸‹ï¼Œå¼ºè¡Œè®°ä¸€ä¸‹è¿™ä¸ªä¸œè¥¿å§ã€‚</p>

<p>Normalizationåˆ™æ˜¯åœ¨ä¹‹å‰å¾ˆå¸¸è§çš„å½’ä¸€åŒ–æ•°æ®çš„æ‰‹æ®µï¼Œèƒ½å¤ŸåŠ å¿«è®­ç»ƒçš„é€Ÿåº¦ï¼Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œä¹Ÿèƒ½è®©è®­ç»ƒæ•°æ®çœ‹èµ·æ¥æ›´åŠ è§„åˆ™ã€‚</p>

<p>ä½†åœ¨transformerä¸­ï¼Œè¿›è¡ŒNormalizationçš„æ‰‹æ®µå¹¶éä¹‹å‰æåˆ°çš„Batch Normalizationï¼Œè€Œæ˜¯ä¸€ç§æ–°çš„Normalizationæ–¹å¼ï¼Œç§°ä¸ºLayer Normalizationã€‚äºŒè€…çš„å·®åˆ«å¦‚ä¸‹å›¾æ‰€ç¤º</p>

<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/normal.png" alt="pic from internet" /></p>

<p>Layer Normalizationæ˜¯åœ¨åŒä¸€ä¸ªæ ·æœ¬ä¸­ä¸åŒç¥ç»å…ƒä¹‹é—´è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€ŒBatch Normalizationæ˜¯åœ¨åŒä¸€ä¸ªbatchä¸­ä¸åŒæ ·æœ¬ä¹‹é—´çš„åŒä¸€ä½ç½®çš„ç¥ç»å…ƒä¹‹é—´è¿›è¡Œå½’ä¸€åŒ–ã€‚</p>

<p>Batch Normalizationæ˜¯å¯¹äºç›¸åŒçš„ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½†æ˜¯å’±ä»¬NLPä¸­è¾“å…¥çš„éƒ½æ˜¯è¯å‘é‡ï¼Œä¸€ä¸ª300ç»´çš„è¯å‘é‡ï¼Œå•ç‹¬å»åˆ†æå®ƒçš„æ¯ä¸€ç»´æ˜¯æ²¡æœ‰æ„ä¹‰åœ°ï¼Œåœ¨æ¯ä¸€ç»´ä¸Šè¿›è¡Œå½’ä¸€åŒ–ä¹Ÿæ˜¯é€‚åˆåœ°ï¼Œå› æ­¤è¿™é‡Œé€‰ç”¨çš„æ˜¯Layer Normalizationã€‚</p>

<p>è§£å†³äº†Addï¼†Normalizeçš„é—®é¢˜ï¼Œæˆ‘ä»¬è¿˜é¢ä¸´ç€æœ€åä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯å‰é¦ˆç¥ç»ç½‘ç»œFeed-Forward Networksçš„é—®é¢˜ã€‚</p>

<p>è¿™ä¸€éƒ¨åˆ†å°±æ¯”è¾ƒç†Ÿæ‚‰äº†ï¼Œåœ¨ä¹‹å‰çš„CNNåˆ†æå›¾ç‰‡ä¸­æˆ‘ä»¬æ›¾ç»ç”¨è¿‡2dçš„convï¼Œè¿™æ¬¡ç”±äºå•è¯çš„å½¢å¼ä¸ºvectorï¼Œæ¢æˆ1då°±å¥½äº†ï¼Œæ¢æ±¤ä¸æ¢è¯ã€‚è¿™ä¸€éƒ¨åˆ†çš„åˆ†ææ”¾åˆ°ä¹‹åçš„ä»£ç è§£æéƒ¨åˆ†å§ã€‚</p>

<p>åˆ°è¿™é‡Œå¯¹encoder layerçš„åˆ†æå°±å·®ä¸å¤šç»“æŸäº†ï¼Œè‡³äºencoderï¼Œå°±æ˜¯æ•°ä¸ªencoder layeré¦–å°¾ç›¸è¿ï¼Œæ— ä»–ã€‚</p>

<p>å…¶å®decoderçš„ç»“æ„ä¸encoderçš„ç»“æ„ç±»ä¼¼ï¼Œå”¯ä¸€å¤šå‡ºæ¥çš„ä¸€éƒ¨åˆ†å°±æ˜¯å…¶ä¸­åŒ…å«maskæ“ä½œã€‚</p>

<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/decoder.png" alt="pic from internet" /></p>

<p>maskæ“ä½œç®€è€Œè¨€ä¹‹å°±æ˜¯å¯¹æ•°æ®è¿›è¡ŒæŸç§æ„ä¹‰ä¸Šçš„è¦†ç›–ï¼Œä¸è¦è®©æ¨¡å‹æ¥è§¦åˆ°å¤šä½™æˆ–æ˜¯é”™è¯¯åœ°ä¿¡æ¯ï¼Œå¯¹è®­ç»ƒè¿‡ç¨‹é€ æˆå½±å“ã€‚</p>

<p>maskåˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯é’ˆå¯¹paddingéƒ¨åˆ†çš„maskï¼Œç”±äºè¾“å…¥çš„å¥å­çš„é•¿åº¦çš„ä¸ç»Ÿä¸€æ€§ï¼Œæˆ‘ä»¬éœ€è¦paddingæ¥è¿›è¡Œè¡¥å…¨ï¼Œä½¿æ•´ä¸ªå¥å­è®­ç»ƒé›†å¯ä»¥ç»„æˆä¸€ä¸ªçŸ©é˜µï¼Œä½†åœ¨åç»­è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç¥ç»ç½‘ç»œå¹¶ä¸çŸ¥é“å“ªä¸€éƒ¨åˆ†æ˜¯çœŸå®çš„æ•°æ®ï¼Œå“ªä¸€éƒ¨åˆ†æ˜¯è¡¥çš„paddingï¼Œä¸ºäº†é˜²æ­¢paddingä¸Šçš„æ•°æ®å¯¹ç¥ç»ç½‘ç»œçš„è®­ç»ƒé€ æˆå½±å“ï¼Œæ‰€ä»¥å¯¹å…¶è¿›è¡Œmaskæ“ä½œè¦†ç›–ã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯ç»™åœ¨è¾ƒçŸ­çš„åºåˆ—åé¢å¡«å…… 0ã€‚ä½†æ˜¯å¦‚æœè¾“å…¥çš„åºåˆ—å¤ªé•¿ï¼Œåˆ™æ˜¯æˆªå–å·¦è¾¹çš„å†…å®¹ï¼ŒæŠŠå¤šä½™çš„ç›´æ¥èˆå¼ƒã€‚å› ä¸ºè¿™äº›å¡«å……çš„ä½ç½®ï¼Œå…¶å®æ˜¯æ²¡ä»€ä¹ˆæ„ä¹‰çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„attentionæœºåˆ¶ä¸åº”è¯¥æŠŠæ³¨æ„åŠ›æ”¾åœ¨è¿™äº›ä½ç½®ä¸Šï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è¿›è¡Œä¸€äº›å¤„ç†ã€‚</p>

<p>å…·ä½“çš„åšæ³•æ˜¯ï¼ŒæŠŠè¿™äº›ä½ç½®çš„å€¼åŠ ä¸Šä¸€ä¸ªéå¸¸å¤§çš„è´Ÿæ•°(è´Ÿæ— ç©·)ï¼Œè¿™æ ·çš„è¯ï¼Œç»è¿‡ softmaxï¼Œè¿™äº›ä½ç½®çš„æ¦‚ç‡å°±ä¼šæ¥è¿‘0ï¼ï¼ˆåœ¨ä¸‹å›¾çš„ä¾‹å­ä¸­ï¼ŒçŸ©é˜µä¸º1çš„ä½ç½®ä¸ºmaskè¦è¦†ç›–çš„ä½ç½®ï¼‰</p>

<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/padding.png" alt="pic from internet" /></p>

<p>ç¬¬äºŒéƒ¨åˆ†æ˜¯sequence maskï¼Œsequence mask æ˜¯ä¸ºäº†ä½¿å¾— decoder ä¸èƒ½çœ‹è§æœªæ¥çš„ä¿¡æ¯ã€‚å¯¹äºä¸€ä¸ªåºåˆ—ï¼Œåœ¨ time_step ä¸º t çš„æ—¶åˆ»ï¼Œæˆ‘ä»¬çš„è§£ç è¾“å‡ºåº”è¯¥åªèƒ½ä¾èµ–äº t æ—¶åˆ»ä¹‹å‰çš„è¾“å‡ºï¼Œè€Œä¸èƒ½ä¾èµ– t ä¹‹åçš„è¾“å‡ºã€‚å› æ­¤æˆ‘ä»¬éœ€è¦æƒ³ä¸€ä¸ªåŠæ³•ï¼ŒæŠŠ t ä¹‹åçš„ä¿¡æ¯ç»™éšè—èµ·æ¥ã€‚è¿™åœ¨è®­ç»ƒçš„æ—¶å€™æœ‰æ•ˆï¼Œå› ä¸ºè®­ç»ƒçš„æ—¶å€™æ¯æ¬¡æˆ‘ä»¬æ˜¯å°†targetæ•°æ®å®Œæ•´è¾“å…¥è¿›decoderä¸­åœ°ï¼Œé¢„æµ‹æ—¶ä¸éœ€è¦ï¼Œé¢„æµ‹çš„æ—¶å€™æˆ‘ä»¬åªèƒ½å¾—åˆ°å‰ä¸€æ—¶åˆ»é¢„æµ‹å‡ºçš„è¾“å‡ºã€‚</p>

<p>é‚£ä¹ˆå…·ä½“æ€ä¹ˆåšå‘¢ï¼Ÿä¹Ÿå¾ˆç®€å•ï¼šäº§ç”Ÿä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œä¸Šä¸‰è§’çš„å€¼å…¨ä¸º0ã€‚æŠŠè¿™ä¸ªçŸ©é˜µä½œç”¨åœ¨æ¯ä¸€ä¸ªåºåˆ—ä¸Šï¼Œå°±å¯ä»¥è¾¾åˆ°æˆ‘ä»¬çš„ç›®çš„ã€‚</p>

<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/sequence.png" alt="pic from internet" /></p>

<p>å‰©ä¸‹çš„ä¸åŒï¼Œå°±åªå‰©ä¸‹åœ¨decoderçš„è¾“å‡ºéƒ¨åˆ†äº†ï¼Œdecoderçš„è¾“å‡ºéƒ¨åˆ†ä¹Ÿå°±æ˜¯æ•´ä¸ªçš„transformerçš„è¾“å‡ºéƒ¨åˆ†ã€‚</p>

<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/output.png" alt="pic from internet" /></p>

<p>Outputå¦‚å›¾ä¸­æ‰€ç¤ºï¼Œé¦–å…ˆç»è¿‡ä¸€æ¬¡çº¿æ€§å˜æ¢ï¼Œç„¶åSoftmaxå¾—åˆ°è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åé€šè¿‡è¯å…¸ï¼Œè¾“å‡ºæ¦‚ç‡æœ€å¤§çš„å¯¹åº”çš„å•è¯ä½œä¸ºæˆ‘ä»¬çš„é¢„æµ‹è¾“å‡ºã€‚</p>

<p>åˆ°è¿™é‡Œtransformerçš„æ‰€æœ‰éƒ¨åˆ†ä¹Ÿå°±å¤§æ¦‚è¯´äº†ä¸€éäº†ã€‚ä¸‹é¢è¿™å¼ å›¾å„¿è¿˜æŒºå½¢è±¡çš„ã€‚</p>

<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/transformer.gif" alt="pic from internet" /></p>

<p>æ‡‚äº†å—ï¼Ÿæ‡‚äº†ï¼Œä½†æ²¡å®Œå…¨æ‡‚ï¼Œä¸‹é¢å†ç»“åˆç€ä»£ç çœ‹ä¸€çœ‹å§ï¼Œçº¸ä¸Šå¾—æ¥ç»ˆè§‰æµ…ï¼Œè¿˜å¾—æ‰“å¼€VScodeã€‚</p>

<h2 id="ä»£ç è§£æéƒ¨åˆ†">ä»£ç è§£æéƒ¨åˆ†</h2>

<p>ä¾èµ–çš„librarieså¦‚ä¸‹</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">math</span>
</code></pre></div></div>

<p>æ’…batch</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_batch</span><span class="p">(</span><span class="n">sentences</span><span class="p">):</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="p">[[</span><span class="n">src_vocab</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">split</span><span class="p">()]]</span>
    <span class="n">output_batch</span> <span class="o">=</span> <span class="p">[[</span><span class="n">tgt_vocab</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">split</span><span class="p">()]]</span>
    <span class="n">target_batch</span> <span class="o">=</span> <span class="p">[[</span><span class="n">tgt_vocab</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">split</span><span class="p">()]]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">input_batch</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">output_batch</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">target_batch</span><span class="p">)</span>
<span class="c1"># æˆ‘ä»¬è¾“å…¥çš„æ‰€æœ‰å¥å­éƒ½è¦æŒ‰ç…§[input,output,target]æ ¼å¼è¿›è¡Œè¾“å…¥
</span></code></pre></div></div>
<p>é¦–å…ˆè¯´mainå‡½æ•°å§</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>

    <span class="c1">## å¥å­çš„è¾“å…¥éƒ¨åˆ†ï¼Œå¯ä»¥è¾“å…¥å¤šç»„å¥å­ï¼Œä¸è¿‡è¦æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¿›è¡Œè¾“å…¥
</span>    <span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s">'ich mochte ein bier P'</span><span class="p">,</span> <span class="s">'S i want a beer'</span><span class="p">,</span> <span class="s">'i want a beer E'</span><span class="p">]</span>


    <span class="c1"># Transformer Parameters
</span>    <span class="c1"># Padding Should be Zero
</span>    <span class="c1">## æ„å»ºè¯è¡¨
</span>    <span class="n">src_vocab</span> <span class="o">=</span> <span class="p">{</span><span class="s">'P'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'ich'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'mochte'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s">'ein'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">'bier'</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span> <span class="c1"># è¾“å…¥è¯è¡¨
</span>    <span class="n">src_vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">src_vocab</span><span class="p">)</span>

    <span class="n">tgt_vocab</span> <span class="o">=</span> <span class="p">{</span><span class="s">'P'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'i'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'want'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s">'a'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">'beer'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">'S'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s">'E'</span><span class="p">:</span> <span class="mi">6</span><span class="p">}</span> <span class="c1"># è¾“å‡ºè¯è¡¨
</span>    <span class="n">tgt_vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tgt_vocab</span><span class="p">)</span>

    <span class="n">src_len</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># length of source
</span>    <span class="n">tgt_len</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># length of target
</span>
    <span class="c1">## æ¨¡å‹å‚æ•°
</span>    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>  <span class="c1"># Embedding Sizeï¼Œå°†æ¯ä¸€ä¸ªè¯embedå…¥512dimä¸­
</span>    <span class="n">d_ff</span> <span class="o">=</span> <span class="mi">2048</span>  <span class="c1"># FeedForward dimensionï¼Œç¥ç»ç½‘ç»œæ·±åº¦
</span>    <span class="n">d_k</span> <span class="o">=</span> <span class="n">d_v</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># dimension of K(=Q), V
</span>    <span class="n">n_layers</span> <span class="o">=</span> <span class="mi">6</span>  <span class="c1"># number of Encoder of Decoder Layer
</span>    <span class="n">n_heads</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># number of heads in Multi-Head Attention
</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">()</span> <span class="c1"># åˆå§‹åŒ–
</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span> <span class="c1"># regressionå¸¸ç”¨loss function
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span> <span class="c1"># æ— è„‘Adam Optimizer
</span>
    <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">target_batch</span> <span class="o">=</span> <span class="n">make_batch</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span> <span class="c1"># æ’…batchï¼Œä¸è¿‡è¿™ä¸ªä¾‹å­ä¸­åªæœ‰ä¸€ä¸ªä¾‹å­ï¼Œä¹Ÿå°±æ²¡batchä¸batchçš„è¯´æ³•äº†
</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># åˆå§‹åŒ–gradian
</span>        <span class="n">outputs</span><span class="p">,</span> <span class="n">enc_self_attns</span><span class="p">,</span> <span class="n">dec_self_attns</span><span class="p">,</span> <span class="n">dec_enc_attns</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">.</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Epoch:'</span><span class="p">,</span> <span class="s">'%04d'</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="s">'cost ='</span><span class="p">,</span> <span class="s">'{:.6f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># å›æº¯
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># æ­¥è¿›
</span>
</code></pre></div></div>

<p>è¿™ä¸è¿‡åªæ˜¯ä¸€ä¸ªç©ºæ¶å­ï¼Œä¸ï¼Œè¿ç©ºæ¶å­éƒ½æ®µä¸ä¸Šï¼Œå…ˆçœ‹ä¸€ä¸‹transformeræ¶æ„å§</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">()</span>  <span class="c1">## ç¼–ç å±‚
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">()</span>  <span class="c1">## è§£ç å±‚
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1">## è¾“å‡ºå±‚ d_model æ˜¯æˆ‘ä»¬è§£ç å±‚æ¯ä¸ªtokenè¾“å‡ºçš„ç»´åº¦å¤§å°ï¼Œä¹‹åä¼šåšä¸€ä¸ª tgt_vocab_size å¤§å°çš„softmaxï¼Œæ ¹æ®æ¦‚ç‡å¤§å°é€‰æ‹©å•è¯
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">):</span>
        <span class="c1">## è¿™é‡Œæœ‰ä¸¤ä¸ªæ•°æ®è¿›è¡Œè¾“å…¥ï¼Œä¸€ä¸ªæ˜¯enc_inputs å½¢çŠ¶ä¸º[batch_size, src_len]ï¼Œä¸»è¦æ˜¯ä½œä¸ºç¼–ç æ®µçš„è¾“å…¥ï¼Œä¸€ä¸ªdec_inputsï¼Œå½¢çŠ¶ä¸º[batch_size, tgt_len]ï¼Œä¸»è¦æ˜¯ä½œä¸ºè§£ç ç«¯çš„è¾“å…¥
</span>
        <span class="c1">## enc_inputsä½œä¸ºè¾“å…¥ å½¢çŠ¶ä¸º[batch_size, src_len]ï¼Œè¾“å‡ºç”±è‡ªå·±çš„å‡½æ•°å†…éƒ¨æŒ‡å®šï¼Œæƒ³è¦ä»€ä¹ˆæŒ‡å®šè¾“å‡ºä»€ä¹ˆï¼Œå¯ä»¥æ˜¯å…¨éƒ¨tokensçš„è¾“å‡ºï¼Œå¯ä»¥æ˜¯ç‰¹å®šæ¯ä¸€å±‚çš„è¾“å‡ºï¼›ä¹Ÿå¯ä»¥æ˜¯ä¸­é—´æŸäº›å‚æ•°çš„è¾“å‡ºï¼›
</span>        <span class="c1">## enc_outputså°±æ˜¯ä¸»è¦çš„è¾“å‡ºï¼Œenc_self_attnsæ˜¯QKè½¬ç½®ç›¸ä¹˜ä¹‹åsoftmaxä¹‹åçš„çŸ©é˜µå€¼ï¼Œä»£è¡¨æ¯ä¸ªå•è¯å’Œå…¶ä»–å•è¯ç›¸å…³æ€§ï¼›
</span>        <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_self_attns</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">)</span>

        <span class="c1">## dec_outputs æ˜¯decoderä¸»è¦è¾“å‡ºï¼Œç”¨äºåç»­çš„linearæ˜ å°„ï¼› dec_self_attnsç±»æ¯”äºenc_self_attns æ˜¯æŸ¥çœ‹æ¯ä¸ªå•è¯å¯¹decoderä¸­è¾“å…¥çš„å…¶ä½™å•è¯çš„ç›¸å…³æ€§ï¼›dec_enc_attnsæ˜¯decoderä¸­æ¯ä¸ªå•è¯å¯¹encoderä¸­æ¯ä¸ªå•è¯çš„ç›¸å…³æ€§ï¼›
</span>        <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_self_attns</span><span class="p">,</span> <span class="n">dec_enc_attns</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">)</span>

        <span class="c1">## dec_outputsåšæ˜ å°„åˆ°è¯è¡¨çš„æ“ä½œ
</span>        <span class="n">dec_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">projection</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">)</span> <span class="c1"># dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]
</span>        <span class="k">return</span> <span class="n">dec_logits</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dec_logits</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">enc_self_attns</span><span class="p">,</span> <span class="n">dec_self_attns</span><span class="p">,</span> <span class="n">dec_enc_attns</span>
</code></pre></div></div>
<p>è¿™ç»ˆäºèƒ½çœ‹å‡ºä¸€ç‚¹æ¶å­çš„æ ·å­äº†ï¼Œé‚£æˆ‘ä»¬å°±æŒ‰ç…§ä»£ç çš„é¡ºåºæ¥ç€å¾€ä¸‹å“ï¼Œå…ˆçœ‹encoder</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Encoder éƒ¨åˆ†åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼šè¯å‘é‡embeddingï¼Œä½ç½®ç¼–ç éƒ¨åˆ†ï¼Œæ³¨æ„åŠ›å±‚åŠåç»­çš„å‰é¦ˆç¥ç»ç½‘ç»œ
</span>
<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">src_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1">## è¿™ä¸ªå…¶å®å°±æ˜¯å»å®šä¹‰ç”Ÿæˆä¸€ä¸ªçŸ©é˜µï¼Œå¤§å°æ˜¯ src_vocab_size * d_model
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span> <span class="c1">## ä½ç½®ç¼–ç æƒ…å†µï¼Œè¿™é‡Œæ˜¯å›ºå®šçš„æ­£ä½™å¼¦å‡½æ•°
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">EncoderLayer</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span> <span class="c1">## ä½¿ç”¨ModuleListå¯¹å¤šä¸ªencoderè¿›è¡Œå †å 
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">):</span>
        <span class="c1">## è¿™é‡Œæˆ‘ä»¬çš„ enc_inputs å½¢çŠ¶æ˜¯ï¼š [batch_size x source_len]
</span>
        <span class="c1">## ä¸‹é¢è¿™ä¸ªä»£ç é€šè¿‡src_embï¼Œè¿›è¡Œç´¢å¼•å®šä½ï¼Œenc_outputsè¾“å‡ºå½¢çŠ¶æ˜¯[batch_size, src_len, d_model]
</span>        <span class="n">enc_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">src_emb</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">)</span>

        <span class="c1">## è¿™é‡Œå°±æ˜¯ä½ç½®ç¼–ç ï¼ŒæŠŠä¸¤è€…ç›¸åŠ æ”¾å…¥åˆ°äº†è¿™ä¸ªå‡½æ•°é‡Œé¢ï¼Œä»è¿™é‡Œå¯ä»¥å»çœ‹ä¸€ä¸‹ä½ç½®ç¼–ç å‡½æ•°çš„å®ç°ï¼›
</span>        <span class="n">enc_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_emb</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1">##get_attn_pad_maskæ˜¯ä¸ºäº†å¾—åˆ°å¥å­ä¸­padçš„ä½ç½®ä¿¡æ¯ï¼Œç»™åˆ°æ¨¡å‹åé¢ï¼Œåœ¨è®¡ç®—è‡ªæ³¨æ„åŠ›å’Œäº¤äº’æ³¨æ„åŠ›çš„æ—¶å€™å»æ‰padç¬¦å·çš„å½±å“
</span>        <span class="n">enc_self_attn_mask</span> <span class="o">=</span> <span class="n">get_attn_pad_mask</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">)</span>
        <span class="n">enc_self_attns</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="c1">## ä¾æ¬¡é€šè¿‡nä¸ªencoder layerï¼ŒæŠŠæ¯ä¸€å±‚çš„è¾“å‡ºå½“ä½œä¸‹ä¸€å±‚çš„è¾“å…¥
</span>            <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_self_attn</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_self_attn_mask</span><span class="p">)</span>
            <span class="n">enc_self_attns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">enc_self_attn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_self_attns</span>
</code></pre></div></div>
<p>ä¸‹é¢æˆ‘ä»¬ä¾æ¬¡æ¥çœ‹ä¸€ä¸‹ä¸Šé¢æåˆ°çš„å‡ ä¸ªå‡½æ•°</p>
<h3 id="positionalencoding">PositionalEncoding</h3>
<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/f1.png" alt="pic from internet" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1">## å…¶å®å°±æ˜¯ç…§ç€ä¸Šé¢ç»™çš„é‚£ä¸ªæœ‰sinï¼Œcosçš„å…¬å¼å¤ç°
</span>        <span class="c1">## ä»ç†è§£æ¥è®²ï¼Œéœ€è¦æ³¨æ„çš„å°±æ˜¯å¶æ•°å’Œå¥‡æ•°åœ¨å…¬å¼ä¸Šæœ‰ä¸€ä¸ªå…±åŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬ä½¿ç”¨logå‡½æ•°æŠŠæ¬¡æ–¹æ‹¿ä¸‹æ¥ï¼Œæ–¹ä¾¿è®¡ç®—
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><span class="c1">## è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯pe[:, 0::2]è¿™ä¸ªç”¨æ³•ï¼Œå°±æ˜¯ä»0å¼€å§‹åˆ°æœ€åé¢ï¼Œè¡¥é•¿ä¸º2ï¼Œå…¶å®ä»£è¡¨çš„å°±æ˜¯å¶æ•°ä½ç½®
</span>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><span class="c1">##è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯pe[:, 1::2]è¿™ä¸ªç”¨æ³•ï¼Œå°±æ˜¯ä»1å¼€å§‹åˆ°æœ€åé¢ï¼Œè¡¥é•¿ä¸º2ï¼Œå…¶å®ä»£è¡¨çš„å°±æ˜¯å¥‡æ•°ä½ç½®
</span>        <span class="c1">## ä¸Šé¢ä»£ç è·å–ä¹‹åå¾—åˆ°çš„pe:[max_len*d_model]
</span>
        <span class="c1">## ä¸‹é¢è¿™ä¸ªä»£ç ä¹‹åï¼Œæˆ‘ä»¬å¾—åˆ°çš„peå½¢çŠ¶æ˜¯ï¼š[max_len*1*d_model]
</span>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'pe'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>  <span class="c1">## å®šä¸€ä¸ªç¼“å†²åŒºï¼Œå…¶å®ç®€å•ç†è§£ä¸ºè¿™ä¸ªå‚æ•°ä¸æ›´æ–°å°±å¯ä»¥
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">"""
        x: [seq_len, batch_size, d_model]
        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="p">:]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="get_attn_pad_mask">get_attn_pad_mask</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## æ¯”å¦‚è¯´ï¼Œæˆ‘ç°åœ¨çš„å¥å­é•¿åº¦æ˜¯5ï¼Œåœ¨åé¢æ³¨æ„åŠ›æœºåˆ¶çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬åœ¨è®¡ç®—å‡ºæ¥QKè½¬ç½®é™¤ä»¥æ ¹å·ä¹‹åï¼Œsoftmaxä¹‹å‰ï¼Œæˆ‘ä»¬å¾—åˆ°çš„å½¢çŠ¶ len_input * len*input  ä»£è¡¨æ¯ä¸ªå•è¯å¯¹å…¶ä½™åŒ…å«è‡ªå·±çš„å•è¯çš„å½±å“åŠ›
</span>
<span class="c1">## æ‰€ä»¥è¿™é‡Œæˆ‘éœ€è¦æœ‰ä¸€ä¸ªåŒç­‰å¤§å°å½¢çŠ¶çš„çŸ©é˜µï¼Œå‘Šè¯‰æˆ‘å“ªä¸ªä½ç½®æ˜¯PADéƒ¨åˆ†ï¼Œä¹‹ååœ¨è®¡ç®—è®¡ç®—softmaxä¹‹å‰ä¼šæŠŠè¿™é‡Œç½®ä¸ºæ— ç©·å¤§ï¼›
</span>
<span class="c1">## ä¸€å®šéœ€è¦æ³¨æ„çš„æ˜¯è¿™é‡Œå¾—åˆ°çš„çŸ©é˜µå½¢çŠ¶æ˜¯batch_size x len_q x len_kï¼Œæˆ‘ä»¬æ˜¯å¯¹kä¸­çš„padç¬¦å·è¿›è¡Œæ ‡è¯†ï¼Œå¹¶æ²¡æœ‰å¯¹kä¸­çš„åšæ ‡è¯†ï¼Œå› ä¸ºæ²¡å¿…è¦
</span>
<span class="c1">## seq_q å’Œ seq_k ä¸ä¸€å®šä¸€è‡´ï¼Œåœ¨äº¤äº’æ³¨æ„åŠ›ï¼Œqæ¥è‡ªè§£ç ç«¯ï¼Œkæ¥è‡ªç¼–ç ç«¯ï¼Œæ‰€ä»¥å‘Šè¯‰æ¨¡å‹ç¼–ç è¿™è¾¹padç¬¦å·ä¿¡æ¯å°±å¯ä»¥ï¼Œè§£ç ç«¯çš„padä¿¡æ¯åœ¨äº¤äº’æ³¨æ„åŠ›å±‚æ˜¯æ²¡æœ‰ç”¨åˆ°çš„ï¼›
</span>
<span class="k">def</span> <span class="nf">get_attn_pad_mask</span><span class="p">(</span><span class="n">seq_q</span><span class="p">,</span> <span class="n">seq_k</span><span class="p">):</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">len_q</span> <span class="o">=</span> <span class="n">seq_q</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">len_k</span> <span class="o">=</span> <span class="n">seq_k</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
    <span class="c1"># eq(zero) is PAD token
</span>    <span class="n">pad_attn_mask</span> <span class="o">=</span> <span class="n">seq_k</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># batch_size x 1 x len_k, one is masking
</span>    <span class="k">return</span> <span class="n">pad_attn_mask</span><span class="p">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">len_k</span><span class="p">)</span>  <span class="c1"># batch_size x len_q x len_k
</span></code></pre></div></div>

<h3 id="encoderlayer">EncoderLayer</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">enc_self_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">()</span> <span class="c1"># å¤šå¤´è‡ªæ³¨æ„åŠ›
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">pos_ffn</span> <span class="o">=</span> <span class="n">PoswiseFeedForwardNet</span><span class="p">()</span> <span class="c1"># å‰é¦ˆç¥ç»ç½‘ç»œ
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_self_attn_mask</span><span class="p">):</span>
        <span class="c1">## ä¸‹é¢è¿™ä¸ªå°±æ˜¯åšè‡ªæ³¨æ„åŠ›å±‚ï¼Œè¾“å…¥æ˜¯enc_inputsï¼Œå½¢çŠ¶æ˜¯[batch_size x seq_len_q x d_model] éœ€è¦æ³¨æ„çš„æ˜¯æœ€åˆå§‹çš„QKVçŸ©é˜µæ˜¯ç­‰åŒäºè¿™ä¸ªè¾“å…¥çš„
</span>        <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">enc_self_attn</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_self_attn_mask</span><span class="p">)</span> <span class="c1"># è·å¾—å½¼æ­¤ç›¸å…³æ€§
</span>        <span class="n">enc_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_ffn</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">)</span> <span class="c1"># enc_outputs: [batch_size x len_q x d_model]
</span>        <span class="k">return</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">attn</span>

</code></pre></div></div>

<h3 id="multiheadattention">MultiHeadAttention</h3>
<p>è¿™ä¹Ÿå°±æ˜¯ä¹‹å‰é•¿ç¯‡å¤§è®ºæ¢è®¨çš„Self-attentionçš„ä¸»ä½“éƒ¨åˆ†</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1">## è¾“å…¥è¿›æ¥çš„QKVæ˜¯ç›¸ç­‰çš„ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨æ˜ å°„linearåšä¸€ä¸ªæ˜ å°„å¾—åˆ°å‚æ•°çŸ©é˜µWq, Wk,Wv
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">W_Q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_K</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_V</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_v</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_heads</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">):</span>

        <span class="c1">## è¿™ä¸ªå¤šå¤´åˆ†ä¸ºè¿™å‡ ä¸ªæ­¥éª¤ï¼Œé¦–å…ˆæ˜ å°„åˆ†å¤´ï¼Œç„¶åè®¡ç®—atten_scoresï¼Œç„¶åè®¡ç®—atten_value;
</span>        <span class="c1">##è¾“å…¥è¿›æ¥çš„æ•°æ®å½¢çŠ¶ï¼š Q: [batch_size x len_q x d_model], K: [batch_size x len_k x d_model], V: [batch_size x len_k x d_model]
</span>        <span class="n">residual</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Q</span><span class="p">,</span> <span class="n">Q</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># (B, S, D) -proj-&gt; (B, S, D) -split-&gt; (B, S, H, W) -trans-&gt; (B, H, S, W)
</span>
        <span class="c1">##ä¸‹é¢è¿™ä¸ªå°±æ˜¯å…ˆæ˜ å°„ï¼Œååˆ†å¤´ï¼›ä¸€å®šè¦æ³¨æ„çš„æ˜¯qå’Œkåˆ†å¤´ä¹‹åç»´åº¦æ˜¯ä¸€è‡´é¢ï¼Œæ‰€ä»¥ä¸€çœ‹è¿™é‡Œéƒ½æ˜¯dk
</span>        <span class="n">q_s</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_Q</span><span class="p">(</span><span class="n">Q</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_k</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># q_s: [batch_size x n_heads x len_q x d_k]
</span>        <span class="n">k_s</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_K</span><span class="p">(</span><span class="n">K</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_k</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># k_s: [batch_size x n_heads x len_k x d_k]
</span>        <span class="n">v_s</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_V</span><span class="p">(</span><span class="n">V</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_v</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># v_s: [batch_size x n_heads x len_k x d_v]
</span>
        <span class="c1">## è¾“å…¥è¿›è¡Œçš„attn_maskå½¢çŠ¶æ˜¯ batch_size x len_q x len_kï¼Œç„¶åç»è¿‡ä¸‹é¢è¿™ä¸ªä»£ç å¾—åˆ° æ–°çš„attn_mask : [batch_size x n_heads x len_q x len_k]ï¼Œå°±æ˜¯æŠŠpadä¿¡æ¯é‡å¤äº†nä¸ªå¤´ä¸Š
</span>        <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


        <span class="c1">##ç„¶åæˆ‘ä»¬è®¡ç®— ScaledDotProductAttention è¿™ä¸ªå‡½æ•°ï¼Œå¾—åˆ°çš„ç»“æœæœ‰ä¸¤ä¸ªï¼šcontext: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q x len_k]
</span>        <span class="n">context</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">()(</span><span class="n">q_s</span><span class="p">,</span> <span class="n">k_s</span><span class="p">,</span> <span class="n">v_s</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">)</span> <span class="c1"># context: [batch_size x len_q x n_heads * d_v]
</span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">output</span> <span class="o">+</span> <span class="n">residual</span><span class="p">),</span> <span class="n">attn</span> <span class="c1"># output: [batch_size x len_q x d_model]
</span>
</code></pre></div></div>

<h3 id="scaleddotproductattention">ScaledDotProductAttention</h3>
<p><img src="http://commcheck396.github.io/blog/assets/img/2022_2_14/encoderlayer.png" alt="pic from internet" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ScaledDotProductAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ScaledDotProductAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">):</span>
        <span class="c1">## è¾“å…¥è¿›æ¥çš„ç»´åº¦åˆ†åˆ«æ˜¯ [batch_size x n_heads x len_q x d_k]  Kï¼š [batch_size x n_heads x len_k x d_k]  V: [batch_size x n_heads x len_k x d_v]
</span>        <span class="c1">##é¦–å…ˆç»è¿‡matmulå‡½æ•°å¾—åˆ°çš„scoreså½¢çŠ¶æ˜¯ : [batch_size x n_heads x len_q x len_k]
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>

        <span class="c1">## ç„¶åå…³é”®è¯åœ°æ–¹æ¥äº†ï¼Œä¸‹é¢è¿™ä¸ªå°±æ˜¯ç”¨åˆ°äº†æˆ‘ä»¬ä¹‹å‰é‡ç‚¹è®²çš„attn_maskï¼ŒæŠŠè¢«maskçš„åœ°æ–¹ç½®ä¸ºæ— é™å°ï¼Œsoftmaxä¹‹ååŸºæœ¬å°±æ˜¯0ï¼Œå¯¹qçš„å•è¯ä¸èµ·ä½œç”¨
</span>        <span class="n">scores</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span> <span class="c1"># Fills elements of self tensor with value where mask is one.
</span>        <span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">context</span><span class="p">,</span> <span class="n">attn</span>
</code></pre></div></div>

<h3 id="poswisefeedforwardnet">PoswiseFeedForwardNet</h3>
<p>å¸¸è§„æ“ä½œï¼ŒåŒå±‚conv</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PoswiseFeedForwardNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoswiseFeedForwardNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">inputs</span> <span class="c1"># inputs : [batch_size, len_q, d_model]
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">output</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">output</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span>
</code></pre></div></div>
<p>encoderå·®ä¸å¤šå°±è¿™ä¹ˆå¤šï¼Œä¸‹é¢æ˜¯decoderï¼Œå’Œencoderç±»ä¼¼ï¼Œä»…æœ‰å°‘é‡è¡¥å……</p>
<h3 id="decoder">Decoder</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tgt_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">DecoderLayer</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">):</span> <span class="c1"># dec_inputs : [batch_size x target_len]
</span>        <span class="n">dec_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tgt_emb</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>  <span class="c1"># [batch_size, tgt_len, d_model]
</span>        <span class="n">dec_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_emb</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># [batch_size, tgt_len, d_model]
</span>
        <span class="c1">## get_attn_pad_mask è‡ªæ³¨æ„åŠ›å±‚çš„æ—¶å€™çš„pad éƒ¨åˆ†
</span>        <span class="n">dec_self_attn_pad_mask</span> <span class="o">=</span> <span class="n">get_attn_pad_mask</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">)</span>

        <span class="c1">## get_attn_subsequent_mask è¿™ä¸ªåšçš„æ˜¯è‡ªæ³¨æ„å±‚çš„maskéƒ¨åˆ†ï¼Œå°±æ˜¯å½“å‰å•è¯ä¹‹åçœ‹ä¸åˆ°ï¼Œä½¿ç”¨ä¸€ä¸ªä¸Šä¸‰è§’ä¸º1çš„çŸ©é˜µ
</span>        <span class="n">dec_self_attn_subsequent_mask</span> <span class="o">=</span> <span class="n">get_attn_subsequent_mask</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>

        <span class="c1">## ä¸¤ä¸ªçŸ©é˜µç›¸åŠ ï¼Œå¤§äº0çš„ä¸º1ï¼Œä¸å¤§äº0çš„ä¸º0ï¼Œä¸º1çš„åœ¨ä¹‹åå°±ä¼šè¢«fillåˆ°æ— é™å°
</span>        <span class="n">dec_self_attn_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gt</span><span class="p">((</span><span class="n">dec_self_attn_pad_mask</span> <span class="o">+</span> <span class="n">dec_self_attn_subsequent_mask</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>


        <span class="c1">## è¿™ä¸ªåšçš„æ˜¯äº¤äº’æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„maskçŸ©é˜µï¼Œencçš„è¾“å…¥æ˜¯kï¼Œæˆ‘å»çœ‹è¿™ä¸ªké‡Œé¢å“ªäº›æ˜¯padç¬¦å·ï¼Œç»™åˆ°åé¢çš„æ¨¡å‹
</span>        <span class="n">dec_enc_attn_mask</span> <span class="o">=</span> <span class="n">get_attn_pad_mask</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">)</span>

        <span class="n">dec_self_attns</span><span class="p">,</span> <span class="n">dec_enc_attns</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_self_attn</span><span class="p">,</span> <span class="n">dec_enc_attn</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">dec_self_attn_mask</span><span class="p">,</span> <span class="n">dec_enc_attn_mask</span><span class="p">)</span>
            <span class="n">dec_self_attns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">dec_self_attn</span><span class="p">)</span>
            <span class="n">dec_enc_attns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">dec_enc_attn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_self_attns</span><span class="p">,</span> <span class="n">dec_enc_attns</span>
</code></pre></div></div>

<h3 id="decoderlayer">DecoderLayer</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dec_self_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dec_enc_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pos_ffn</span> <span class="o">=</span> <span class="n">PoswiseFeedForwardNet</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">dec_self_attn_mask</span><span class="p">,</span> <span class="n">dec_enc_attn_mask</span><span class="p">):</span>
        <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_self_attn</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_self_attn</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_self_attn_mask</span><span class="p">)</span>
        <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_enc_attn</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dec_enc_attn</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">dec_enc_attn_mask</span><span class="p">)</span>
        <span class="n">dec_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_ffn</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_self_attn</span><span class="p">,</span> <span class="n">dec_enc_attn</span>
</code></pre></div></div>

<h3 id="get_attn_subsequent_mask">get_attn_subsequent_mask</h3>
<p>è·å–subsequent mask</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_attn_subsequent_mask</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
    <span class="s">"""
    seq: [batch_size, tgt_len]
    """</span>
    <span class="n">attn_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">seq</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">seq</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">seq</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>
    <span class="c1"># attn_shape: [batch_size, tgt_len, tgt_len]
</span>    <span class="n">subsequence_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">attn_shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># ç”Ÿæˆä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µ
</span>    <span class="n">subsequence_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">subsequence_mask</span><span class="p">).</span><span class="n">byte</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">subsequence_mask</span>  <span class="c1"># [batch_size, tgt_len, tgt_len]
</span></code></pre></div></div>

<p>æ€»ç®—æ˜¯ä»å¤´åˆ°å°¾æ¢³ç†äº†ä¸€éï¼Œæœ‰äº›åœ°æ–¹è¿˜æ˜¯æœ‰äº›æ¨¡æ£±ä¸¤å¯ï¼Œæ²¡å…³ç³»ï¼Œä¹‹åå†ç ”ç©¶ç ”ç©¶ï¼Œç´¯äº†ä¸æƒ³çœ‹äº†ï¼Œæ”¶æ‹¾ä¸œè¥¿å»äº†ã€‚</p>

<p>å¯’å‡è¿‡å¾—çœŸå¿«ï¼Œæ˜å¤©åˆå¼€å­¦äº†ï¼Œä¸çŸ¥é“å¼€å­¦è¿˜æœ‰æ²¡æœ‰æ—¶é—´å†™åšå®¢ï¼Œç´¯å•Šã€‚</p>

<p>å”‰ï¼ŒåŠ æ²¹å§ï¼Œåå¤§ç‰¢å»å–½ï¼Œéš¾é¡¶ğŸ˜­ã€‚</p>
:ET